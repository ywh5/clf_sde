{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:02.195158Z",
     "start_time": "2025-05-27T08:27:02.175835Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from cnews_loader import *"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:02.206564Z",
     "start_time": "2025-05-27T08:27:02.196325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置数据读取、模型、结果保存路径\n",
    "base_dir = '/cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "save_dir = os.path.join('checkpoints', 'textcnn')\n",
    "save_path = os.path.join(save_dir, 'best_validation')"
   ],
   "id": "f946a8ef8f9fec5",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:11.413319Z",
     "start_time": "2025-05-27T08:27:02.234931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_contents, train_labels = read_file('./cnews/cnews.train.txt')\n",
    "test_contents, test_labels = read_file('./cnews/cnews.test.txt')\n",
    "val_counts = Counter(train_labels)\n",
    "val_counts"
   ],
   "id": "d4baa40340c9962",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'体育': 5000,\n",
       "         '娱乐': 5000,\n",
       "         '家居': 5000,\n",
       "         '房产': 5000,\n",
       "         '教育': 5000,\n",
       "         '时尚': 5000,\n",
       "         '时政': 5000,\n",
       "         '游戏': 5000,\n",
       "         '科技': 5000,\n",
       "         '财经': 5000})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:30:02.843746Z",
     "start_time": "2025-05-27T08:30:02.700058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "#去除文本中的表情字符（只保留中英文和数字）\n",
    "def clear_character(sentence):\n",
    "    pattern1= '\\[.*?\\]'     \n",
    "    pattern2 = re.compile('[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]')   \n",
    "    line1=re.sub(pattern1,'',sentence)\n",
    "    line2=re.sub(pattern2,'',line1)   \n",
    "    new_sentence=''.join(line2.split()) #去除空白\n",
    "    return new_sentence\n",
    "\n",
    "# train_text=list(map(lambda s: clear_character(s), train_contents))\n",
    "train_text = list(map(lambda s: clear_character(str(s)), train_contents))\n",
    "test_text=list(map(lambda s: clear_character(str(s)), test_contents))"
   ],
   "id": "afa617e7dcee8fbc",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m new_sentence\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# train_text=list(map(lambda s: clear_character(s), train_contents))\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m train_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mclear_character\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_contents\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m test_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m s: clear_character(s), test_contents))\n",
      "Cell \u001B[1;32mIn[38], line 12\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(s)\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m new_sentence\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# train_text=list(map(lambda s: clear_character(s), train_contents))\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m train_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m s: \u001B[43mclear_character\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m, train_contents))\n\u001B[0;32m     13\u001B[0m test_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m s: clear_character(s), test_contents))\n",
      "Cell \u001B[1;32mIn[38], line 6\u001B[0m, in \u001B[0;36mclear_character\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m      4\u001B[0m pattern1\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m[.*?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m     \n\u001B[0;32m      5\u001B[0m pattern2 \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[^\u001B[39m\u001B[38;5;130;01m\\u4e00\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;130;01m\\u9fa5\u001B[39;00m\u001B[38;5;124m^a-z^A-Z^0-9]\u001B[39m\u001B[38;5;124m'\u001B[39m)   \n\u001B[1;32m----> 6\u001B[0m line1\u001B[38;5;241m=\u001B[39m\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern1\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m line2\u001B[38;5;241m=\u001B[39mre\u001B[38;5;241m.\u001B[39msub(pattern2,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,line1)   \n\u001B[0;32m      8\u001B[0m new_sentence\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(line2\u001B[38;5;241m.\u001B[39msplit()) \u001B[38;5;66;03m#去除空白\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python310\\lib\\re.py:209\u001B[0m, in \u001B[0;36msub\u001B[1;34m(pattern, repl, string, count, flags)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msub\u001B[39m(pattern, repl, string, count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    203\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001B[39;00m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m    a callable, it's passed the Match object and must return\u001B[39;00m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;124;03m    a replacement string to be used.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 209\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:24.277061Z",
     "start_time": "2025-05-27T08:27:21.279435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jieba\n",
    "train_seg_text=list(map(lambda s: jieba.lcut(s), train_text))\n",
    "test_seg_text=list(map(lambda s: jieba.lcut(s), test_text))"
   ],
   "id": "11d56e411ada840c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:24.360011Z",
     "start_time": "2025-05-27T08:27:24.279621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取停用词\n",
    "stop_words_path = \"./百度停用词列表.txt\"\n",
    "def get_stop_words():\n",
    "    file = open(stop_words_path, 'rb').read().decode('gbk').split('\\r\\n')\n",
    "    return set(file)\n",
    "stopwords = get_stop_words()\n",
    "\n",
    "# 去掉文本中的停用词\n",
    "def drop_stopwords(line, stopwords):\n",
    "    line_clean = []\n",
    "    for word in line:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        line_clean.append(word)\n",
    "    return line_clean\n",
    "\n",
    "train_st_text=list(map(lambda s: drop_stopwords(s,stopwords), train_seg_text))\n",
    "test_st_text=list(map(lambda s: drop_stopwords(s,stopwords), test_seg_text))"
   ],
   "id": "f0654b3b699582d6",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:24.444267Z",
     "start_time": "2025-05-27T08:27:24.360893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "label_train_id=le.transform(train_labels)\n",
    "label_test_id=le.transform(test_labels)"
   ],
   "id": "90d36c7e54027439",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:24.832821Z",
     "start_time": "2025-05-27T08:27:24.444267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_c_text=list(map(lambda s: ' '.join(s), train_st_text))\n",
    "test_c_text=list(map(lambda s: ' '.join(s), test_st_text))\n",
    "tfidf_model = TfidfVectorizer(binary=False,token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "train_Data = tfidf_model.fit_transform(train_c_text)\n",
    "test_Data = tfidf_model.transform(test_c_text)\n"
   ],
   "id": "d8ada48382e3acfa",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:25.849233Z",
     "start_time": "2025-05-27T08:27:24.834073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "'''LR模型分类训练'''\n",
    "classifier=LogisticRegression()\n",
    "classifier.fit(train_Data, label_train_id)\n",
    "pred = classifier.predict(test_Data)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test_id, pred,digits=4))\n"
   ],
   "id": "32cc6df280deffb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.0450    0.0861      1000\n",
      "           1     0.8235    0.0140    0.0275      1000\n",
      "           2     0.1047    0.9890    0.1894      1000\n",
      "           3     0.7778    0.0070    0.0139      1000\n",
      "           4     1.0000    0.0030    0.0060      1000\n",
      "           5     0.0000    0.0000    0.0000      1000\n",
      "           6     0.8333    0.0050    0.0099      1000\n",
      "           7     0.6930    0.0790    0.1418      1000\n",
      "           8     0.9718    0.3450    0.5092      1000\n",
      "           9     0.8000    0.0040    0.0080      1000\n",
      "\n",
      "    accuracy                         0.1491     10000\n",
      "   macro avg     0.7004    0.1491    0.0992     10000\n",
      "weighted avg     0.7004    0.1491    0.0992     10000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:40.507872Z",
     "start_time": "2025-05-27T08:27:25.850293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.75,),\n",
    "#     'tfidf__stop_words':('english',stopwords),\n",
    "    'tfidf__norm':('l2',),\n",
    "    'tfidf__use_idf':(True,),\n",
    "    'tfidf__smooth_idf':(True,),\n",
    "    'tfidf__max_features':(None,),\n",
    "#     'tfidf__ngram_range':((1, 1), (1, 2),(2, 2)),  # unigrams or bigrams\n",
    "\n",
    "#     'clf__max_iter': (20,),\n",
    "    'clf__penalty': ('l1','l2'),\n",
    "    # 'clf__tol': (0.0001,0.00001,0.000001),\n",
    "    'clf__solver': ( 'liblinear','saga',),\n",
    "}\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_c_text, label_train_id)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n"
   ],
   "id": "fe51f705851506c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__penalty': ('l1', 'l2'),\n",
      " 'clf__solver': ('liblinear', 'saga'),\n",
      " 'tfidf__max_df': (0.75,),\n",
      " 'tfidf__max_features': (None,),\n",
      " 'tfidf__norm': ('l2',),\n",
      " 'tfidf__smooth_idf': (True,),\n",
      " 'tfidf__use_idf': (True,)}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "done in 14.643s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:40.519455Z",
     "start_time": "2025-05-27T08:27:40.509211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ],
   "id": "8550be2fc714d398",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.118\n",
      "Best parameters set:\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "\ttfidf__max_df: 0.75\n",
      "\ttfidf__max_features: None\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__smooth_idf: True\n",
      "\ttfidf__use_idf: True\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:55.568871Z",
     "start_time": "2025-05-27T08:27:40.524629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.75,),\n",
    "#     'tfidf__stop_words':('english',stopwords),\n",
    "    'tfidf__norm':('l2',),\n",
    "    'tfidf__use_idf':(True,),\n",
    "    'tfidf__smooth_idf':(True,),\n",
    "    'tfidf__max_features':(None,),\n",
    "    # 'tfidf__ngram_range':((1, 1), (1, 2),(2, 2)),  # unigrams or bigrams\n",
    "\n",
    "#     'clf__max_iter': (20,),\n",
    "    'clf__penalty': ('l2',),\n",
    "    'clf__C':(0.8,0.9,1.0,1.1,),\n",
    "    'clf__tol': (0.001,0.0001,0.00001,0.000001,),\n",
    "    'clf__solver': ( 'liblinear',),\n",
    "}\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_c_text, label_train_id)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n"
   ],
   "id": "cc2ae7570dc53a84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.8, 0.9, 1.0, 1.1),\n",
      " 'clf__penalty': ('l2',),\n",
      " 'clf__solver': ('liblinear',),\n",
      " 'clf__tol': (0.001, 0.0001, 1e-05, 1e-06),\n",
      " 'tfidf__max_df': (0.75,),\n",
      " 'tfidf__max_features': (None,),\n",
      " 'tfidf__norm': ('l2',),\n",
      " 'tfidf__smooth_idf': (True,),\n",
      " 'tfidf__use_idf': (True,)}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "done in 15.022s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:55.587635Z",
     "start_time": "2025-05-27T08:27:55.570041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ],
   "id": "9aeb5678508d77f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.118\n",
      "Best parameters set:\n",
      "\tclf__C: 1.0\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "\tclf__tol: 0.001\n",
      "\ttfidf__max_df: 0.75\n",
      "\ttfidf__max_features: None\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__smooth_idf: True\n",
      "\ttfidf__use_idf: True\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:56.510295Z",
     "start_time": "2025-05-27T08:27:55.589396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''LR模型分类训练'''\n",
    "classifier=LogisticRegression(C=1.1)\n",
    "classifier.fit(train_Data, label_train_id)\n",
    "pred = classifier.predict(test_Data)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_test_id, pred,digits=4))\n"
   ],
   "id": "201ee20df131f253",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.0450    0.0861      1000\n",
      "           1     0.8235    0.0140    0.0275      1000\n",
      "           2     0.1047    0.9890    0.1894      1000\n",
      "           3     0.7778    0.0070    0.0139      1000\n",
      "           4     1.0000    0.0030    0.0060      1000\n",
      "           5     0.0000    0.0000    0.0000      1000\n",
      "           6     0.8333    0.0050    0.0099      1000\n",
      "           7     0.6930    0.0790    0.1418      1000\n",
      "           8     0.9718    0.3450    0.5092      1000\n",
      "           9     0.8000    0.0040    0.0080      1000\n",
      "\n",
      "    accuracy                         0.1491     10000\n",
      "   macro avg     0.7004    0.1491    0.0992     10000\n",
      "weighted avg     0.7004    0.1491    0.0992     10000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:27:56.523705Z",
     "start_time": "2025-05-27T08:27:56.511508Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a5bfec48c329a5ac",
   "outputs": [],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
